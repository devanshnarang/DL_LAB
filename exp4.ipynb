{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11157182,"sourceType":"datasetVersion","datasetId":6961395}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import DataLoader, Dataset\nfrom collections import Counter\n\n# Load dataset\ndf = pd.read_csv(\"/kaggle/input/poems-dataset/poems-100.csv\")\ntext = \" \".join(df[\"text\"].astype(str).tolist()).lower()\nwords = text.split()\n\n# Vocabulary creation\nvocab = list(set(words))\nword_to_idx = {word: i for i, word in enumerate(vocab)}\nidx_to_word = {i: word for word, i in word_to_idx.items()}\n\n# One-Hot Encoding Function\ndef one_hot_encode(sequence, vocab_size):\n    encoded = np.zeros((len(sequence), vocab_size), dtype=np.float32)\n    for i, idx in enumerate(sequence):\n        encoded[i, idx] = 1.0\n    return encoded\n\n# Dataset Preparation\nclass PoetryDataset(Dataset):\n    def __init__(self, words, word_to_idx, context_size=5):\n        self.context_size = context_size\n        self.data = []\n        for i in range(len(words) - context_size):\n            self.data.append((words[i:i+context_size], words[i+context_size]))\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        context, target = self.data[index]\n        context_idxs = [word_to_idx[word] for word in context]\n        target_idx = word_to_idx[target]\n        return torch.tensor(context_idxs, dtype=torch.long), torch.tensor(target_idx, dtype=torch.long)\n\n# Model Definition\nclass RNNModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n        super(RNNModel, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, vocab_size)\n    \n    def forward(self, x):\n        x = self.embedding(x)\n        out, _ = self.rnn(x)\n        out = self.fc(out[:, -1, :])\n        return out\n\n# Training Function\ndef train_model(model, dataset, epochs=10, lr=0.01):\n    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    \n    for epoch in range(epochs):\n        total_loss = 0\n        for inputs, targets in dataloader:\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(dataloader)}\")\n\n# Prepare dataset\ndataset = PoetryDataset(words, word_to_idx)\n\n# Train One-Hot Model\nvocab_size = len(vocab)\nembedding_dim = 50\nhidden_dim = 128\nmodel = RNNModel(vocab_size, embedding_dim, hidden_dim)\ntrain_model(model, dataset)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-25T06:36:26.930244Z","iopub.execute_input":"2025-03-25T06:36:26.930591Z","iopub.status.idle":"2025-03-25T06:38:12.820369Z","shell.execute_reply.started":"2025-03-25T06:36:26.930567Z","shell.execute_reply":"2025-03-25T06:38:12.819205Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 8.596567235973797\nEpoch 2, Loss: 8.267568258233705\nEpoch 3, Loss: 7.751467957071005\nEpoch 4, Loss: 7.3649318893699105\nEpoch 5, Loss: 7.021360802002743\nEpoch 6, Loss: 6.834417656507406\nEpoch 7, Loss: 6.636956230634859\nEpoch 8, Loss: 6.427139007412909\nEpoch 9, Loss: 6.378462053363104\nEpoch 10, Loss: 6.149745958125915\n","output_type":"stream"}],"execution_count":4}]}
